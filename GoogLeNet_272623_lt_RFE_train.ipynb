{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  版本確認\n",
    "#PYTHON 3.7\n",
    "#tensorflow==2.0.0\n",
    "#keras==2.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)\n",
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import 函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import keras\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv2D,MaxPool2D,MaxPooling2D,GlobalAveragePooling2D,AveragePooling2D,Input, concatenate\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import itertools\n",
    "from keras.models import Model,load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA資料讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(k1_n,RFE):\n",
    "    if k1_n == 1:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt1lt2'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt1lt2.dat'\n",
    "    if k1_n == 2:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt1_lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt1_lt3.dat'    \n",
    "    if k1_n == 3:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt2lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt2lt3.dat'\n",
    "    if k1_n == 4:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt1lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K1-272623_peptide_nm1_lt1lt3.dat'\n",
    "\n",
    "    if k1_n == 5:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt1lt2'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt1lt2.dat'\n",
    "    if k1_n == 6:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt1_lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt1_lt3.dat'    \n",
    "    if k1_n == 7:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt2lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt2lt3.dat'\n",
    "    if k1_n == 8:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt1lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K3-272623_peptide_nm1_lt1lt3.dat'\n",
    "\n",
    "    if k1_n == 9:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt1lt2'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt1lt2.dat'\n",
    "    if k1_n == 10:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt1_lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt1_lt3.dat'    \n",
    "    if k1_n == 11:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt2lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt2lt3.dat'\n",
    "    if k1_n == 12:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt1lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-Ushuffle-seed0-K5-272623_peptide_nm1_lt1lt3.dat'\n",
    "        \n",
    "    if k1_n == 13:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt1lt2'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt1lt2.dat'\n",
    "    if k1_n == 14:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt1_lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt1_lt3.dat'\n",
    "    if k1_n == 15:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt2lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt2lt3.dat'\n",
    "    if k1_n == 16:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt1lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-balance-272623_peptide_nm1_lt1lt3.dat'\n",
    "\n",
    "    if k1_n == 17:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt1lt2'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt1lt2.dat'\n",
    "    if k1_n == 18:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt1_lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt1_lt3.dat'    \n",
    "    if k1_n == 19:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt2lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt2lt3.dat'\n",
    "    if k1_n == 20:\n",
    "        ver = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt1lt3'\n",
    "        data = '272623-10.5-11.0-train-Valid-cdhit60-LeftRight-seed0-272623_peptide_nm1_lt1lt3.dat'\n",
    "    \n",
    "\n",
    "        \n",
    "    data_path = './272623/data/'\n",
    "    ver = ver+'_RFE_'+str(RFE)\n",
    "    train_path = data_path+ver+'.dat'\n",
    "    return train_path,ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE_HOT 編碼轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料前處理\n",
    "1.data資料分割\n",
    "2.np.array.reshape\n",
    "3.one_hot轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_data(train_path):\n",
    "    y_train= list()\n",
    "    X_train= list()\n",
    "    f = open(train_path,'r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    data_train = s.split(\"\\n\")\n",
    "    data_train.pop()\n",
    "    random.shuffle(data_train)\n",
    "    #print(data_train[0])\n",
    "    for i in data_train:\n",
    "        data_t = i.split(\" \",1)\n",
    "        y_train.append(data_t[0])\n",
    "        X_train.append(data_t[1])\n",
    "        #print(data_t[0])\n",
    "        #print(data_t[1])\n",
    "    data_x_train = list()\n",
    "    for i in range(0,len(X_train)):\n",
    "        data_t2 = X_train[i].split(\" \")\n",
    "        data_t2.pop()\n",
    "        #print(len(data_t2))\n",
    "        data_x_train.append(np.zeros( len(data_t2) ))\n",
    "        for i_2 in data_t2:\n",
    "            temp = i_2.split(\":\")\n",
    "            temp[0] = int(temp[0])-1\n",
    "            temp[1] = float(temp[1])\n",
    "            data_x_train[i][temp[0]] = temp[1]\n",
    "            #print(type(temp[0]) )\n",
    "            #print(type(temp[1]) )\n",
    "    X_train = np.array(data_x_train)\n",
    "    y_train_label = np.array(y_train)\n",
    "        \n",
    "    #X_test = np.array(data_x_test)\n",
    "    #y_test_label = np.array(y_test)\n",
    "    \n",
    "    #print(X_train.shape)\n",
    "    #print(y_train_label.shape)\n",
    "    #print(X_test.shape)\n",
    "    #print(y_test_label.shape)\n",
    "    \n",
    "    len_type = int( (len(data_x_train[0]))/2) \n",
    "\n",
    "    \n",
    "    X_train = np.reshape(X_train, [-1,len_type,2,1])\n",
    "    #X_test = np.reshape(X_test, [-1,2,len_type,len_type])\n",
    "    #X_test = np.swapaxes(X_test,1,3)\n",
    "    y_train_onehot = to_categorical(y_train_label)\n",
    "    #y_test_onehot = to_categorical(y_test_label)\n",
    "    print(X_train.shape)\n",
    "    print(y_train_onehot.shape)\n",
    "    #print(X_test.shape)\n",
    "    #print(y_test_onehot.shape)\n",
    "    return len_type,X_train,y_train_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=3,name=bn_name)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inception(x,nb_filter_para):\n",
    "    (branch1,branch2,branch3,branch4)= nb_filter_para\n",
    "    branch1x1 = Conv2D(branch1[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n",
    "\n",
    "    branch3x3 = Conv2D(branch2[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n",
    "    branch3x3 = Conv2D(branch2[1],(3,3), padding='same',strides=(1,1),name=None)(branch3x3)\n",
    "\n",
    "    branch5x5 = Conv2D(branch3[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n",
    "    branch5x5 = Conv2D(branch3[1],(1,1), padding='same',strides=(1,1),name=None)(branch5x5)\n",
    "\n",
    "    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n",
    "    branchpool = Conv2D(branch4[0],(1,1),padding='same',strides=(1,1),name=None)(branchpool)\n",
    "\n",
    "    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV1(width, height, depth, classes):\n",
    "    \n",
    "    inpt = Input(shape=(width,height,depth))\n",
    "\n",
    "    x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same')\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "    x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same')\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "\n",
    "    x = Inception(x,[(64,),(96,128),(16,32),(32,)]) #Inception 3a 28x28x256\n",
    "    x = Inception(x,[(128,),(128,192),(32,96),(64,)]) #Inception 3b 28x28x480\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) #14x14x480\n",
    "\n",
    "    x = Inception(x,[(192,),(96,208),(16,48),(64,)]) #Inception 4a 14x14x512\n",
    "    x = Inception(x,[(160,),(112,224),(24,64),(64,)]) #Inception 4a 14x14x512\n",
    "    x = Inception(x,[(128,),(128,256),(24,64),(64,)]) #Inception 4a 14x14x512\n",
    "    x = Inception(x,[(112,),(144,288),(32,64),(64,)]) #Inception 4a 14x14x528\n",
    "    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 4a 14x14x832\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) #7x7x832\n",
    "\n",
    "    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 5a 7x7x832\n",
    "    x = Inception(x,[(384,),(192,384),(48,128),(128,)]) #Inception 5b 7x7x1024\n",
    "\n",
    "    #Using AveragePooling replace flatten\n",
    "    x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x)\n",
    "    x =Flatten()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1000,activation='relu')(x)\n",
    "    x = Dense(classes,activation='softmax')(x)\n",
    "    \n",
    "    model=Model(input=inpt,output=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 1000\n",
    "num_classes = 2\n",
    "weight_decay = 1e-5\n",
    "RFE_A = [100,200,300,400,500]\n",
    "for rfe in RFE_A:\n",
    "    for k in range(1,5):\n",
    "        train_path,ver = get_data(k,rfe)\n",
    "        len_type,X_train,y_train_onehot = deal_data(train_path)\n",
    "        patience = [10,20,30,40,50]\n",
    "        for p in patience:\n",
    "            model_path = './model/GoogLeNet_RFE/'+ver+'p='+str(p)+'.hdf5'\n",
    "            InceptionV1_model = InceptionV1(len_type,2,1,2)\n",
    "            #InceptionV1_model.summary()\n",
    "            InceptionV1_model.compile(optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "            es=EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=p,\n",
    "                verbose=0,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            mc=ModelCheckpoint(\n",
    "                model_path,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=1,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='max',\n",
    "                period=1\n",
    "            )\n",
    "            \n",
    "        \n",
    "            history = InceptionV1_model.fit(\n",
    "                X_train,\n",
    "                y_train_onehot,\n",
    "                batch_size=batch_size,\n",
    "                epochs = epochs,\n",
    "                callbacks=[es,mc],\n",
    "                validation_split = 0.1,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            with open(model_path+'_train_log.txt',\"w\") as f:\n",
    "                f.write('Patience:' + str(p) + '\\n')\n",
    "                f.write('Train accuracy:' + str(max(history.history['accuracy']) ) + '\\n')\n",
    "                f.write('Val accuracy:' + str(max(history.history['val_accuracy']) ) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 1000\n",
    "num_classes = 2\n",
    "weight_decay = 1e-5\n",
    "RFE_A = [100,200,300,400,500]\n",
    "for rfe in RFE_A:\n",
    "    for k in range(5,13):\n",
    "        train_path,ver = get_data(k,rfe)\n",
    "        len_type,X_train,y_train_onehot = deal_data(train_path)\n",
    "        patience = [10,20,30,40,50]\n",
    "        for p in patience:\n",
    "            model_path = './model/GoogLeNet_RFE/'+ver+'p='+str(p)+'.hdf5'\n",
    "            InceptionV1_model = InceptionV1(len_type,2,1,2)\n",
    "            #InceptionV1_model.summary()\n",
    "            InceptionV1_model.compile(optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "            es=EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=p,\n",
    "                verbose=0,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            mc=ModelCheckpoint(\n",
    "                model_path,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=1,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='max',\n",
    "                period=1\n",
    "            )\n",
    "            \n",
    "        \n",
    "            history = InceptionV1_model.fit(\n",
    "                X_train,\n",
    "                y_train_onehot,\n",
    "                batch_size=batch_size,\n",
    "                epochs = epochs,\n",
    "                callbacks=[es,mc],\n",
    "                validation_split = 0.1,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            with open(model_path+'_train_log.txt',\"w\") as f:\n",
    "                f.write('Patience:' + str(p) + '\\n')\n",
    "                f.write('Train accuracy:' + str(max(history.history['accuracy']) ) + '\\n')\n",
    "                f.write('Val accuracy:' + str(max(history.history['val_accuracy']) ) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow2.0",
   "language": "python",
   "name": "tensorflow_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
